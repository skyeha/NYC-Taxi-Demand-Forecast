{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/26 01:08:54 WARN Utils: Your hostname, DESKTOP-H6V94HM resolves to a loopback address: 127.0.1.1; using 192.168.0.100 instead (on interface eth0)\n",
      "24/08/26 01:08:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/26 01:08:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1 Data Preprocessing\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config('spark.driver.memory', '4g')\n",
    "    .config('spark.executor.memory', '2g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will shadow the Tan's approach that was mentioned in the MAST30034 2022 Sample Solution to process the feature as well as the final subset of feature that will be used for analysis and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleaning(df):\n",
    "    \"\"\"\n",
    "        This function perform basic cleaning and extraction of the columns of a dataframe\n",
    "    \"\"\"\n",
    "    selected_columns = [\"DATE\",\"WND\",\"TMP\",\"DEW\",\"SLP\"]\n",
    "    df = df.select(*selected_columns)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == \"DATE\":\n",
    "            df = df.filter(\n",
    "                ((\"2023-03-01\" <= F.to_date(F.col(col))) & (F.to_date(F.col(col)) <= \"2023-12-31\")) |\n",
    "                ((\"2024-01-01\" <= F.to_date(F.col(col))) & (F.to_date(F.col(col)) <= \"2024-05-31\")))\n",
    "            continue\n",
    "        df = df.withColumn(col, F.split(F.col(col), \",\"))\n",
    "\n",
    "    # Extracting date and assigning season\n",
    "    df = df.withColumns({\"year\": F.year(F.col(\"DATE\")),\n",
    "                         \"month\": F.month(F.col(\"DATE\")),\n",
    "                         \"day\": F.day(F.col(\"DATE\")),\n",
    "                         \"time\": F.date_format(F.col(\"DATE\"), \"HH:mm:ss\"),\n",
    "                         \"season\": season_assign(F.col(\"DATE\"))})\n",
    "    df = df.drop(F.col(\"DATE\"))\n",
    "\n",
    "    # Selecting the desired item for each column\n",
    "    df = df.withColumn(\"WND\", F.col(\"WND\").getItem(3).cast(DoubleType()))\n",
    "    df = df.withColumn(\"TMP\", F.col(\"TMP\").getItem(0).cast(DoubleType()))\n",
    "    df = df.withColumn(\"DEW\", F.col(\"DEW\").getItem(0).cast(DoubleType()))\n",
    "    df = df.withColumn(\"SLP\", F.col(\"SLP\").getItem(0).cast(DoubleType()))\n",
    "\n",
    "\n",
    "    df = df.withColumnsRenamed({\"WND\": \"wind_speed\", \"TMP\": \"temp\",\n",
    "                                \"DEW\": \"dew_point\", \"SLP\": \"atm_pressure\"})\n",
    "    ordered_cols = [\"season\", \"year\", \"month\", \"day\", \"time\", \"wind_speed\", \"temp\",\n",
    "                    \"dew_point\", \"atm_pressure\"]\n",
    "    \n",
    "    df = df.select(*ordered_cols)\n",
    "    return df\n",
    "\n",
    "def season_assign(col):\n",
    "    \"\"\"\n",
    "        Assign season to entries according to date\n",
    "    \"\"\"\n",
    "    col = F.month(col)\n",
    "    return (F.when((col == 3) | (col == 4) | (col == 5), 1)\\\n",
    "        .when((col == 6) | (col == 7) | (col == 8), 2)\\\n",
    "        .when((col == 9) | (col == 10) | (col == 11), 3)\\\n",
    "        .when((col == 12) | (col == 1) | (col == 2),4))\n",
    "\n",
    "def compute_missing_values(df, col):\n",
    "    \"\"\"\n",
    "        This function replace missing value of a feature by the value of the instance right before it.\n",
    "        This function was inspired by Ming Hui Tan - MAST30034 2022 Sample Solution\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"index\", F.row_number().over(Window.orderBy(F.lit(1))))\n",
    "    window_spec = Window.orderBy(\"index\")\n",
    "\n",
    "    # Creating lag value i.e. value of the preceding row for a specified column\n",
    "    df = df.withColumn(\"prev_value\", F.lag(col).over(window_spec))\n",
    "    df = df.withColumn(col, F.when((F.col(col) == 9999) | (F.col(col) == 99999), F.col(\"prev_value\")).otherwise(F.col(col))).drop(\"prev_value\", \"index\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def rescale_column(df):\n",
    "    \"\"\"\n",
    "        Rescale the values of the field accordingly to the data dictionary\n",
    "    \"\"\"\n",
    "    SCALE_FACTOR = {\"wind_speed\": 10, \"temp\": 10, \"dew_point\": 10, \"atm_pressure\": 10}\n",
    "\n",
    "    for col, factor in SCALE_FACTOR.items():\n",
    "        df = df.withColumn(col, F.round(F.col(col)/factor,2))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2023 weather\n",
    "weather_23 = spark.read.parquet(\"../data/raw/2023-weather\")\n",
    "weather_23 = basic_cleaning(weather_23)\n",
    "\n",
    "# Load 2024 weather\n",
    "weather_24 = spark.read.parquet(\"../data/raw/2024-weather\")\n",
    "weather_24 = basic_cleaning(weather_24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we begin to split the weather data into train and test which will later on be matched with the corresponding train and test data. Train data holds records from March 2023 to Febuary 2024, and test data holds records from March 2024 to May 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = weather_23.union(weather_24.filter(F.col('month') <= 2))\n",
    "df_test = weather_24.filter(F.col('month') >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute missing value by it using it previous records. Some are next to each other, thus iterate the process \n",
    "for i in range(1,6):\n",
    "    for col in [\"wind_speed\", \"temp\", \"dew_point\", \"atm_pressure\"]:\n",
    "        df_train = compute_missing_values(df_train, col)\n",
    "        df_test = compute_missing_values(df_test, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rescaling the column accordingly to the data dictionary and drop any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = rescale_column(df_train)\n",
    "df_train = df_train.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = rescale_column(df_test)\n",
    "df_test = df_test.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the data into a conformable structure that can be used for merging later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df_train = df_train.groupBy(\"month\")\\\n",
    "    .agg(\n",
    "        F.round(F.mean(\"temp\"), 2).alias(\"avg_temp\"),\n",
    "        F.round(F.mean(\"wind_speed\"),2).alias(\"avg_windspeed\"),\n",
    "        F.round(F.mean(\"dew_point\"),2).alias(\"avg_dew\"),\n",
    "        F.round(F.mean(\"atm_pressure\"),2).alias(\"avg_pressure\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df_test = df_test.groupBy(\"month\")\\\n",
    "    .agg(\n",
    "        F.round(F.mean(\"temp\"), 2).alias(\"avg_temp\"),\n",
    "        F.round(F.mean(\"wind_speed\"),2).alias(\"avg_windspeed\"),\n",
    "        F.round(F.mean(\"dew_point\"),2).alias(\"avg_dew\"),\n",
    "        F.round(F.mean(\"atm_pressure\"),2).alias(\"avg_pressure\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clean data into the curated layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = \"../data/curated\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df_train.write.parquet(\"../data/curated/weather_train\")\n",
    "agg_df_test.write.parquet(\"../data/curated/weather_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unemployment rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is simple and there no need for data cleaning. Some transformation are apply to the `Label` feature so that it contain the month only, which we will use later on for merging. We also filter the data based and split the data that matches our research range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df = pd.read_csv(\"../data/landing/unemployment_rate.csv\")\n",
    "emp_df = emp_df[['Label', 'Value']]\n",
    "\n",
    "# Filters enties outside of desired range\n",
    "emp_df = emp_df.loc[(emp_df['Label'] != '2023 Jan') & \n",
    "           (emp_df['Label'] != '2023 Feb') &\n",
    "           (emp_df['Label'] != '2024 Jun') &\n",
    "           (emp_df['Label'] != '2024 Jul')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18770/1332400904.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  emp_df_train['Label'] = emp_df_train['Label'].replace(month_mapping)\n"
     ]
    }
   ],
   "source": [
    "# Convert format: YYYY M to categorical numerical\n",
    "emp_df_train = emp_df.iloc[ :12, :]\n",
    "month_mapping = {\n",
    "    '2023 Mar': 3, '2023 Apr': 4, '2023 May': 5,\n",
    "    '2023 Jun': 6, '2023 Jul': 7, '2023 Aug': 8,\n",
    "    '2023 Sep': 9, '2023 Oct': 10, '2023 Nov': 11,\n",
    "    '2023 Dec': 12, '2024 Jan': 1, '2024 Feb': 2\n",
    "}\n",
    "emp_df_train['Label'] = emp_df_train['Label'].replace(month_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18770/124332444.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  emp_df_test['Label'] = emp_df_test['Label'].replace(month_mapping)\n"
     ]
    }
   ],
   "source": [
    "emp_df_test = emp_df.iloc[ 12: , :]\n",
    "month_mapping = {\n",
    "    '2024 Mar': 3, '2024 Apr': 4, '2024 May': 5\n",
    "}\n",
    "emp_df_test['Label'] = emp_df_test['Label'].replace(month_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the `.csv` file as `.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df_train.to_parquet(\"../data/curated/unemp_train.parquet\", index=False)\n",
    "emp_df_test.to_parquet(\"../data/curated/unemp_test.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
